{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "import os\n",
    "from typing import List\n",
    "import geopandas as gpd\n",
    "import plotly_express as px\n",
    "import networkx as nx\n",
    "\n",
    "load_dotenv()\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n",
    "1. Load Aleph entities\n",
    "2. Load Mapstand data\n",
    "3. Add missing geometries to mapstand, drop rows that are not used\n",
    "4. Merge Mapstand with Aleph entities\n",
    "5. Clean it up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_ALEPH = os.environ.get('PATH_ALEPHDATA')\n",
    "PATH_RAW = os.environ.get('PATH_RAW')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_json(entities: List) -> pd.DataFrame:\n",
    "    '''Parses Aleph JSON data\n",
    "    '''\n",
    "    \n",
    "    entity_list = []\n",
    "    \n",
    "    for entity in entities:\n",
    "        data = entity.get('properties')\n",
    "\n",
    "        for key, value in data.items():\n",
    "            if isinstance(value, list):\n",
    "                data.update({key: ','.join(value)})\n",
    "        entity_id = {'id': entity.get('id')}\n",
    "        data.update(entity_id)\n",
    "        entity_list.append(data)\n",
    "    \n",
    "    df = pd.DataFrame(entity_list)\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_entities(path: str, entity: str) -> pd.DataFrame:\n",
    "    '''Load entities from Aleph\n",
    "    (downloaded through alephclient)'''\n",
    "\n",
    "    entities = []\n",
    "    with open(f'{path}{entity}.json', 'r') as file:\n",
    "        for line in file:\n",
    "            entities.append(json.loads(line))\n",
    "\n",
    "    df = parse_json(entities)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Aleph entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import companies\n",
    "\n",
    "companies = load_entities(PATH_ALEPH, 'companies')\n",
    "companies.drop(['notes', 'summary', 'sourceUrl','publisher', 'alias', 'description', \n",
    "                'leiCode', 'parent', 'amountEur'], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "# Import and clean assets\n",
    "\n",
    "assets = load_entities(PATH_ALEPH, 'assets')\n",
    "assets.dropna(subset='description', inplace=True)\n",
    "assets.drop(['title', 'authority', 'contractDate', 'jurisdiction', 'registrationNumber',\n",
    "             'previousName', 'parent', 'leiCode', 'sourceUrl', 'publisher'], axis=1, inplace=True)\n",
    "\n",
    "# Import ownerships\n",
    "\n",
    "ownerships = load_entities(PATH_ALEPH, 'ownerships')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import geometries \n",
    "\n",
    "We have one geosjon with edited data on capacity and installation year, but it's not complete (I know...). So let's get the newest installed and planned windfarm dataset from MapStand\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import geometries with edited data\n",
    "\n",
    "ms = gpd.GeoDataFrame.from_file(PATH_RAW + 'mapstand_final.geojson', geometry='geometry')\n",
    "ms = ms.to_crs(4326)\n",
    "\n",
    "# So import newest for missing geometries\n",
    "\n",
    "msn = gpd.GeoDataFrame.from_file(PATH_RAW + 'mapstand_final_newest_installed.geojson')\n",
    "msp = gpd.GeoDataFrame.from_file(PATH_RAW + 'mapstand_final_newest_planned.geojson')\n",
    "wdz = gpd.GeoDataFrame.from_file(PATH_RAW + 'mapstand_final_wdz.geojson')\n",
    "\n",
    "# Just use the items from WDZ we need\n",
    "\n",
    "ids = ['cf34dea4-5b4a-462c-b8b5-56c1ebcebcc7', '116dd036-d8f3-49b7-b63e-625b0020993e']\n",
    "wdz = wdz[wdz.mps_uuid.isin(ids)].copy()\n",
    "\n",
    "msn = pd.concat([msn, msp])\n",
    "\n",
    "lookup = ['HIRTSHALS HARBOUR', \n",
    "          'WP Q10 / ENECO LUCHTERDUINEN', \n",
    "          'UNITECH ZEFYROS (HYWIND DEMO/KARMOY) - METCENTRE',\n",
    "          'AFLANDSHAGE', \n",
    "          'LEVENMOUTH DEMONSTRATION TURBINE',\n",
    "          'BLYTH',\n",
    "          'BEATRICE DEMONSTRATOR SITE',\n",
    "          'EAST ANGLIA THREE'\n",
    "          ]\n",
    "\n",
    "msn = msn[msn.name.isin(lookup)][:-1].copy()\n",
    "msn = msn.to_crs(4326)\n",
    "\n",
    "# Merge the missing with the right ones\n",
    "\n",
    "ms = pd.concat([ms, msn, wdz])\n",
    "\n",
    "# Create selection of relevant columns\n",
    "\n",
    "cols = ['mps_uuid', 'name', 'cost_in_million', 'year', 'capacity_mw', 'description', 'remarks',\n",
    "        'number_generators',  'installation_year', 'mps_est_elevation_max_m', 'mps_est_elevation_min_m', 'geometry']\n",
    "\n",
    "selection = ms[cols].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge with assets\n",
    "\n",
    "df = pd.merge(assets,\n",
    "             selection,\n",
    "             left_on = 'description',\n",
    "             right_on='mps_uuid',\n",
    "             how='outer')\n",
    "\n",
    "# Clean column names\n",
    "\n",
    "df = df.rename(columns={'name_x': 'name_aleph', \n",
    "                        'name_y': 'name_ms',\n",
    "                        'description_y': 'description',\n",
    "                        'notes': 'status'\n",
    "                        })\n",
    "\n",
    "df.drop('description_x', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean MW\n",
    "\n",
    "df.amount = df.amount.str.replace(' MW', '')\n",
    "df.amount = df.amount.astype('float')\n",
    "df['capacity_mw'] = df['capacity_mw'].fillna(df.amount)\n",
    "\n",
    "# Clean costs\n",
    "df[['amountEur', 'cost_in_million']]\n",
    "df.amountEur = df.amountEur.fillna(df.cost_in_million * 1000000).astype('float')\n",
    "\n",
    "df = df.drop(['cost_in_million', 'amount'], axis=1)\n",
    "\n",
    "# Clean installation year\n",
    "\n",
    "df.year = df.year.fillna(df.installation_year.astype(str).replace('.0', '') + '-01-01')\n",
    "\n",
    "# Clean wind farms we want to leave out and mislabeled wind farm\n",
    "\n",
    "df = df[(df.name_aleph.notna()) & (df.name_aleph != 'DIAMOND OFFSHORE WIND HOLDINGS I BV')].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add simplified status from Marina\n",
    "\n",
    "st_dict = {'OPERATIONAL': \"EXISTING\", \n",
    " 'EXCLUSIVE_DEVELOPMENT_RIGHTS': \"EARLY_PLANS\",\n",
    " 'UNDER_CONSTRUCTION': \"EXISTING\", \n",
    " 'PROJECT_ANNOUNCED': \"EARLY_PLANS\", \n",
    " 'EARLY_PLANNING': \"EARLY_PLANS\",\n",
    "'APPLICATION_SUBMITTED': \"SERIOUS_PLANS\", \n",
    "'CONSENT_AUTHORISED': \"SERIOUS_PLANS\", \n",
    "'EXTENSION_REQUESTED': \"SERIOUS_PLANS\", \n",
    "'CANCELLED': \"\", \n",
    "'ONSHORE - OPERATIONAL': \"\",\n",
    "'DECOMMISSIONED': \"\", \n",
    "'AREA_PROPOSED': \"EARLY_PLANS\"}\n",
    "\n",
    "df['status_simplified'] = df.status.replace(st_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create geodataframe\n",
    "\n",
    "gdf = gpd.GeoDataFrame(df, geometry='geometry', crs=4326)\n",
    "\n",
    "# Reproject so we can calculate area (WGS84/UTM zone 31N is a fine one)\n",
    "\n",
    "gdf = gdf.to_crs(32631)\n",
    "gdf['area_km'] = gdf.geometry.area / 1000000\n",
    "\n",
    "# Write to file\n",
    "gdf.to_file(PATH_ALEPH + 'analysis_v1.geojson', driver='GeoJSON')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "We would like some basic statistics, for instance:\n",
    "1. Comparisons between countries regarding output, area, number of turbines, if possible normalised with population size.\n",
    "2. Temporal trends.\n",
    "3. A detailed overview of company shares in wind farms. For this we would probably need Neo4J to query the ownership graph, or simplify this graph to a table. \n",
    "4. Perform linear regressions on costs of wind farms, because some data is lacking. \n",
    "5. Assess how much power is sold through PPAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First get the population of the countries for normalization, in thousands\n",
    "\n",
    "population = {'de': 83000,\n",
    "              'gb': 68000,\n",
    "              'nl': 18000,\n",
    "              'be': 12000,\n",
    "              'no': 5000,\n",
    "              'dk': 6000}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Capacity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by country and status simplified\n",
    "\n",
    "capacity = gdf.groupby(['country', 'status_simplified']).capacity_mw.sum().reset_index()\n",
    "capacity = capacity[capacity.status_simplified != ''].copy()\n",
    "\n",
    "# Add capacity per 1000 inhabitants\n",
    "\n",
    "capacity['cap_pop'] = capacity.capacity_mw.div(capacity.country.map(population))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "\n",
    "fig = px.bar(capacity, \n",
    "             x='country', \n",
    "             y='cap_pop', \n",
    "             facet_col='status_simplified',\n",
    "             title = 'Energy output per capita (x1000)')\n",
    "fig.for_each_annotation(lambda a: a.update(text=a.text.split(\"=\")[1]))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Capacity over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Groupby year and country\n",
    "\n",
    "c_time = gdf.groupby(['country', 'year']).capacity_mw.sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.area(c_time,\n",
    "              x='year',\n",
    "              y='capacity_mw',\n",
    "              color='country',\n",
    "              title='Projected megawatt production per country per year',\n",
    "             )\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse ownership structure\n",
    "\n",
    "There are several ways to go about this. It would be great if we could (partly) automate the generation of ownership tables, using graphs. We could use neo4j for that, or networkx. A query should look something like this:\n",
    "1. For each asset assign a value of 1\n",
    "2. Travel on an ownership relation and multiply the weight of that relationship, e.g. 1\n",
    "3. At the next node find all ownership relationships\n",
    "4. Travel all relationships and multiply by the weight of that relationship, e.g. .5\n",
    "\n",
    "\n",
    "One of the problems with ownership is that we have some ranges and non-precise values (e.g. 75+). A solution is to define lower and upper values and convert them to weights. So this means we would create a few extra columns (percentage_lower_bound, percentage_upper_bound) and convert them to proper percentages so we can easily use them for multiplication. \n",
    "\n",
    "For now we have to assume that a missing percentage is 100 percent. That will often be te case, but we have to go through the Aleph data again one time to fill in the missing percentages.\n",
    "\n",
    "One promising approach is to use Dijkstra's Algorithm, the shortest path, between companies (source) and assets (targets). Because we're dealing with a directed graph, this should omit any detours because companies have joint ventures in other projects. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean percentages\n",
    "\n",
    "ownerships.percentage = ownerships.percentage.str.replace('+', '-100')\n",
    "ownerships.percentage.fillna('100', inplace=True)\n",
    "ownerships.percentage = ownerships.percentage.astype('str')\n",
    "\n",
    "# Add columns for lower and upper bound\n",
    "\n",
    "ownerships['perc_lower'] = ownerships.percentage.apply(lambda x: float(x.split('-')[0]) / 100 if '-' in x else float(x) / 100)\n",
    "ownerships['perc_higher'] = ownerships.percentage.apply(lambda x: float(x.split('-')[1]) / 100 if '-' in x else float(x) / 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directed graph\n",
    "\n",
    "G = nx.DiGraph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in assets.iterrows():\n",
    "    G.add_node(row.id,\n",
    "               name=row['name'],\n",
    "               status=row.notes,\n",
    "               country=row.country, \n",
    "               costs=row.amountEur, \n",
    "               mps_uuid=row.description,\n",
    "               aleph_url=row.alephUrl\n",
    "               )\n",
    "    \n",
    "for i, row in companies.iterrows():\n",
    "    G.add_node(row.id,\n",
    "               name=row['name'],\n",
    "               country=row.jurisdiction,\n",
    "               registration=row.registrationNumber,\n",
    "               source_url=row.publisherUrl,\n",
    "               aleph_url=row.alephUrl\n",
    "               )\n",
    "    \n",
    "for i, row in ownerships.iterrows():\n",
    "    G.add_edge(row.owner,\n",
    "               row.asset,\n",
    "               weight_lower=row.perc_lower,\n",
    "               weight_upper=row.perc_higher,\n",
    "               percentage=row.percentage,\n",
    "               source=row.publisherUrl,\n",
    "               aleph_url=row.alephUrl,\n",
    "               id=id,\n",
    "               description=row.description\n",
    "               )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get source, target and use Dijkstra's algorithm to get the nodes and relationships inbetween\n",
    "\n",
    "source = 'IBERDROLA SA'\n",
    "target = 'BALTIC EAGLE'\n",
    "\n",
    "s = [n for n, v in nx.get_node_attributes(G, 'name').items() if v == source][0]\n",
    "t = [n for n, v in nx.get_node_attributes(G, 'name').items() if v == target][0]\n",
    "\n",
    "shortest_path = [x for x in nx.dijkstra_path(G, s, t)]\n",
    "\n",
    "window_size = 2\n",
    "weight = 1\n",
    "\n",
    "for i in range(len(shortest_path) - window_size + 1):\n",
    "    node1 = shortest_path[i: i + window_size][0]\n",
    "    node2 = shortest_path[i: i + window_size][1]\n",
    "    e = list(G.edges([node1, node2], data=True))\n",
    "    weight *= e[0][2]['weight_lower']\n",
    "    \n",
    "    # Print outcome\n",
    "    print(f\"Node weight = {e[0][2]['weight_lower']} and cumulative weight = {weight}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO:\n",
    "1. Better output of shortest path, weights and cumulative weight, so we can check the results\n",
    "2. Add logic to run Dijkstra's Algorithm against all companies (sources) and assets (targets)\n",
    "3. Check results:\n",
    "    a. are all relationships in the right direction?\n",
    "    b. do all asset ownerships add up to 1? \n",
    "4. Create a nice table with ownerships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
