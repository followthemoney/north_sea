{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Some remarks\n",
    "\n",
    "There are several ways to import the data. Here are a few options provided.\n",
    "The first option is to get data from WFS services. The second is to download spatial files (gdb, shapefiles). Although the second one is a bit cumbersome, the data provided in the spatial files tend to be cleaner. Also there seem to be limits on the number of rows you can download through the Norwegian WFS. Using the spatial files, I'm more certain I have all the rows available. \n",
    "\n",
    "All relevant data is written to Google Drive and, after some basic cleaning (see below) to a PostGIS instance. A choice needs still to be made where to clean the data. The data is pretty clean right now, and things like field names can be cleaned when querying the data in PostGIS. The values though, need to be cleaned. It might be wiser to do that in Python in stead of PostGIS. I already have some cleaning functions available in another notebook.\n",
    "\n",
    "## TODO:\n",
    "\n",
    "- Refactor code\n",
    "- Create a environment for this project (currently in a general GIS environment) and move some constants to the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract\n",
    "from arcgis.features import FeatureLayerCollection\n",
    "from arcgis.gis import GIS\n",
    "from owslib.wfs import WebFeatureService\n",
    "import requests\n",
    "from fiona import BytesCollection\n",
    "import fiona\n",
    "\n",
    "# Transform\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Load\n",
    "from sqlalchemy import create_engine\n",
    "import psycopg2\n",
    "\n",
    "# General\n",
    "from IPython.display import display\n",
    "import zipfile\n",
    "import io\n",
    "import glob\n",
    "import os\n",
    "import openpyxl\n",
    "import numpy as np\n",
    "\n",
    "# ENV\n",
    "gis = GIS()\n",
    "#POSTGIS_ENGINE = create_engine('postgresql://postgres:postgres@vmhost:5432/north_sea') #If used in arcgis env on parallels\n",
    "POSTGIS_ENGINE = create_engine('postgresql://postgres:postgres@localhost:5432/north_sea')\n",
    "\n",
    "# Set CRS\n",
    "\n",
    "NEW_CRS = 25831\n",
    "NL_CRS = 'EPSG:25831'\n",
    "NO_CRS = 'EPSG:32636'\n",
    "UK_CRS = 'EPSG:4326'\n",
    "\n",
    "# Make sure you have access to the Google Drive\n",
    "\n",
    "PATH_SPATIAL = '/Volumes/GoogleDrive-113330074508532941534/My Drive/Projecten/north_sea_data/Spatial/'\n",
    "PATH_PRODUCTION = '/Volumes/GoogleDrive-113330074508532941534/My Drive/Projecten/north_sea_data/Production/'\n",
    "PATH_COMPANIES = '/Volumes/GoogleDrive-113330074508532941534/My Drive/Projecten/north_sea_data/Companies/'\n",
    "PATH_LICENCES = '/Volumes/GoogleDrive-113330074508532941534/My Drive/Projecten/north_sea_data/Licences/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WFS and ArcGIS REST services (#EPSG for reprojecting)\n",
    "\n",
    "# NETHERLANDS\n",
    "\n",
    "WFS_NL_INFRA = 'https://geo.rijkswaterstaat.nl/services/ogc/gdr/kabels_en_leidingen_noordzee/ows?' # EPSG:25831\n",
    "WFS_NL_LICENCES = 'https://www.nlog.nl/arcgis/services/NLOG_WFS/gdw_ng_wfs_licence_utm/MapServer/WFSServer?' # EPSG:25831\n",
    "#WFS_NL_LICENSES = 'https://www.gdngeoservices.nl/inspire/wfs/olie_en_gasvelden?'\n",
    "WFS_NL_FIELD = 'https://www.nlog.nl/arcgis/services/NLOG_WFS/gdw_ng_wfs_field_utm/MapServer/WFSServer?' # EPSG:25831\n",
    "WFS_NL_WELLBORES = 'https://www.nlog.nl/arcgis/services/NLOG_WFS/gdw_ng_wfs_wll_utm/MapServer/WFSServer?' # EPSG:25831\n",
    "\n",
    "# NORWAY\n",
    "\n",
    "WFS_NO = 'https://factmaps.npd.no/arcgis/services/FactMaps_ogc/3_0_WGS84_z32/MapServer/WFSServer?' #EPSG:32632\n",
    "\n",
    "# UK\n",
    "\n",
    "ARCGIS_UK_INFRA = 'https://data.nstauthority.co.uk/arcgis/rest/services/Public_WGS84/UKCS_Offshore_Infrastructure_WGS84/FeatureServer' # EPSG:4326\n",
    "ARCGIS_UK_CARBON = 'https://data.nstauthority.co.uk/arcgis/rest/services/Public_WGS84/UKCS_CarbonStorage_Licences_WGS84/FeatureServer' # EPSG:4326\n",
    "ARCGIS_UK_CSS_OFFER = 'https://data.nstauthority.co.uk/arcgis/rest/services/Public_WGS84/UKCS_CSOfferAreas_WGS84/FeatureServer' # EPSG:4326\n",
    "ARCGIS_UK_LICENCES = 'https://data.nstauthority.co.uk/arcgis/rest/services/Public_WGS84/UKCS_Licences_WGS84/FeatureServer' # EPSG:4326\n",
    "ARCGIS_UK_WELLBORES = 'https://data.nstauthority.co.uk/arcgis/rest/services/Public_WGS84/UKCS_Wells_WGS84/FeatureServer' # EPSG:4326\n",
    "ARCGIS_UK_LICENCES_HIST = 'https://data.nstauthority.co.uk/arcgis/rest/services/Public_WGS84/UKCS_Licensed_Blocks_History_WGS84/FeatureServer' # EPSG:4326\n",
    "ARCGIS_UK_FIELDS = 'https://data.nstauthority.co.uk/arcgis/rest/services/Public_WGS84/UKCS_Offshore_Fields_WGS84/FeatureServer'\n",
    "\n",
    "# EMODNET\n",
    "\n",
    "EMODNET = 'https://ows.emodnet-humanactivities.eu/wfs?SERVICE%3DWFS&REQUEST%3DGetCapabilities&VERSION=2.0.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some functions to import wfs into a geodataframe with the right crs\n",
    "\n",
    "def wfs2gdf(layer, url, output_format, wfs_version=\"2.0.0\"):\n",
    "    '''\n",
    "    Needs layer, wfs_url and output_format\n",
    "    as input and creates a geodataframe\n",
    "    '''\n",
    "    \n",
    "    params = dict(service='WFS', \n",
    "                  version=wfs_version, \n",
    "                  request='GetFeature', \n",
    "                  typeName=layer, \n",
    "                  outputFormat=output_format)\n",
    "    \n",
    "    if 'xml' in output_format:\n",
    "        with BytesCollection(requests.get(url,params=params).content) as f:\n",
    "            df = gpd.GeoDataFrame.from_features(f)\n",
    "    \n",
    "    elif 'json' in output_format:\n",
    "            r = requests.get(url, params=params)\n",
    "            df = gpd.read_file(r.text)\n",
    "        \n",
    "    return df\n",
    "\n",
    "\n",
    "def get_wfs_layers(url):\n",
    "    '''\n",
    "    Get list of available layers\n",
    "    and their index\n",
    "    '''\n",
    "    \n",
    "    wfs = WebFeatureService(url, version='2.0.0')\n",
    "    \n",
    "    for i, layer in enumerate(wfs.contents):\n",
    "        print(i, layer)\n",
    "        \n",
    "def select_wfs_layer(url, index):\n",
    "    '''\n",
    "    Select a layer by index\n",
    "    '''\n",
    "    wfs = WebFeatureService(url, version='2.0.0')\n",
    "    layer = list(wfs.contents)[index]\n",
    "    \n",
    "    return layer\n",
    "\n",
    "def get_arcgis_layers(url):\n",
    "    '''\n",
    "    Get layers from ArcGIS \n",
    "    FeatureServer\n",
    "    '''\n",
    "    arcgis_layers = FeatureLayerCollection(url)\n",
    "    \n",
    "    for i, layer in enumerate(arcgis_layers.layers):\n",
    "        print(i, layer.properties.name)\n",
    "    \n",
    "def arcgis2gdf(url, layer_index):\n",
    "    '''\n",
    "    Use layer index to import\n",
    "    data from FeatureServer in gdf\n",
    "    '''\n",
    "    \n",
    "    collection = FeatureLayerCollection(url)\n",
    "    layer = collection.layers[layer_index]\n",
    "    \n",
    "    data = layer.query()\n",
    "    \n",
    "    geojson_string = data.to_geojson\n",
    "    geojson_dict = json.loads(geojson_string)\n",
    "    \n",
    "    gdf = gpd.GeoDataFrame.from_features(geojson_dict['features'])\n",
    "    \n",
    "    return gdf\n",
    "\n",
    "def set_crs(df, old_crs, new_crs):\n",
    "    '''\n",
    "    Makes sure there is a crs defined\n",
    "    in a geodataframe and sets it to \n",
    "    the project crs\n",
    "    '''\n",
    "    \n",
    "    if df.crs is None:\n",
    "        df.crs = old_crs\n",
    "    else: \n",
    "        pass\n",
    "    \n",
    "    df.to_crs(epsg=new_crs, inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def clean_values(df, cols):\n",
    "    '''Cleans columns and values\n",
    "    using a dict'''\n",
    "    \n",
    "    df.columns = df.columns.str.lower()\n",
    "    \n",
    "    df = df.rename(columns=col_dict)\n",
    "    \n",
    "    for col in cols:\n",
    "        if col in df.columns:\n",
    "            df[col+'_raw'] = df[col]\n",
    "            df[col] = df[col].str.upper()\n",
    "            df = df.replace({col: value_dict})\n",
    "            \n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    return df\n",
    "\n",
    "def clean_uk(df):\n",
    "    '''clean date columns\n",
    "    in UK files'''\n",
    "    \n",
    "    date_cols = [col for col in df.columns if 'TDT' in col or 'DDT' in col]\n",
    "    if date_cols:\n",
    "        df[date_cols] = df[date_cols].apply(pd.to_datetime, errors='coerce')\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's start with Norway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get gdb from NPD Norway\n",
    "\n",
    "r = requests.get('https://factpages.npd.no/downloads/fgdb/NPD_FactMapsData_v3_0.zip')\n",
    "z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "z.extractall(f'{PATH_SPATIAL}NO/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get list of layers\n",
    "\n",
    "fiona.listlayers(f'{PATH_SPATIAL}NO/NPD_FactMapsData_v3_0.gdb/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = ['COMPANY', \n",
    "          'FACILITY_FUNCTION',\n",
    "          'FIELD_LICENSEE_HST',\n",
    "          'FIELD_OPERATOR_HST',\n",
    "          'FIELD_OWNER_HST',\n",
    "          'FIELD_RESERVES',\n",
    "          'LICENCE_LICENSEE_HST',\n",
    "          'LICENCE_OPER_HST',\n",
    "          'LICENCE_TRANSFER_HST',\n",
    "          'BLOCK',\n",
    "          'FACILITY',\n",
    "          'LICENCE',\n",
    "          'PIPELINE_THIN', \n",
    "          'QUADRANT', \n",
    "          'SUB_AREA',\n",
    "          'WELLBORE',\n",
    "          'FIELD']\n",
    "\n",
    "names = ['no_company', 'no_facility_function', 'no_field_licensee_history',\n",
    "         'no_field_operator_history', 'no_field_owner_history', 'no_field_reserves',\n",
    "         'no_licence_licensee_history', 'no_licence_oper_history', 'no_licence_transfer_history',\n",
    "         'no_block', 'no_facility', 'no_licence', 'no_pipeline_thin', 'no_quadrant',\n",
    "         'no_sub_area', 'no_wellbore', 'no_field']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name,layer in zip(names, layers):\n",
    "    df = gpd.read_file(f'{PATH_SPATIAL}NO/NPD_FactMapsData_v3_0.gdb/', layer=layer)\n",
    "    df.crs = \"EPSG:23032\"\n",
    "    df.columns = df.columns.str.lower()\n",
    "    try:\n",
    "        df.to_postgis(name, POSTGIS_ENGINE, if_exists='append', index=False)\n",
    "    except ValueError:\n",
    "        df = pd.DataFrame(df)\n",
    "        df.to_sql(name, POSTGIS_ENGINE, if_exists='append', index=False)\n",
    "    except AttributeError:\n",
    "        df = df[df['geometry'] != None]\n",
    "        df.to_postgis(name, POSTGIS_ENGINE, if_exists='append', index=False)\n",
    "    print(f'Exported {name} to postgis')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get production data\n",
    "\n",
    "production_yearly_url = 'https://factpages.npd.no/ReportServer_npdpublic?/FactPages/tableview/field_production_yearly&rs:Command=Render&rc:Toolbar=false&rc:Parameters=f&IpAddress=not_used&CultureCode=en&rs:Format=CSV&Top100=false'\n",
    "production_monthly_url = 'https://factpages.npd.no/ReportServer_npdpublic?/FactPages/tableview/field_production_monthly&rs:Command=Render&rc:Toolbar=false&rc:Parameters=f&IpAddress=not_used&CultureCode=en&rs:Format=CSV&Top100=false'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And write to PostGIS\n",
    "\n",
    "production_monthly = pd.read_csv(production_monthly_url)\n",
    "production_yearly = pd.read_csv(production_yearly_url)\n",
    "production_yearly.to_sql('no_production_yearly', POSTGIS_ENGINE, if_exists='append')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean production\n",
    "\n",
    "p = production_monthly.melt(['prfInformationCarrier', 'prfYear', 'prfMonth'], var_name='commodity', value_name='sm3')\n",
    "\n",
    "sm3dict = {'Oil': 1000000,\n",
    "           'Gas': 1000000000,\n",
    "           'NGL': 1000000,\n",
    "           'Condensate': 1000000,\n",
    "           'OeNet': 1000000,\n",
    "           'WaterInField': 1000000}\n",
    "\n",
    "for k, v, in sm3dict.items():\n",
    "    p['sm3'] = np.where(p['commodity'].str.contains(k),\n",
    "                    p['sm3'] * v,\n",
    "                    p['sm3'])\n",
    "    \n",
    "p.commodity = p.commodity.str.replace('prfPrd|NetMillSm3|NetBillSm3|MillSm3', '', regex=True)\n",
    "\n",
    "p = p[p.commodity != 'prfNpdidInformationCarrier']\n",
    "p = p.rename(columns={'prfInformationCarrier': 'field_name',\n",
    "                      'prfYear': 'year',\n",
    "                      'prfMonth': 'month'})\n",
    "\n",
    "p['date'] = pd.to_datetime(p['year'].astype(str) + '-' + p['month'].astype(str) + '-01')\n",
    "p.columns = p.columns.str.lower()\n",
    "p.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.to_sql('no_production_monthly', POSTGIS_ENGINE, if_exists='append')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Then UK..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_uk = 'https://datanstauthority.blob.core.windows.net/external/OpenDataZips/UKCS_OFF_ED50.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get shp from NSTA\n",
    "\n",
    "r = requests.get(url_uk)\n",
    "z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "z.extractall(f'{PATH_SPATIAL}UK/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Surface Points (WGS84)\n",
      "1 Subsea Points (WGS84)\n",
      "2 Pipeline Points (WGS84)\n",
      "3 Pipeline Freespans (WGS84)\n",
      "4 Subsea Linear (WGS84)\n",
      "5 Pipelines Linear (WGS84)\n",
      "6 Surface Points - Removed (WGS84)\n",
      "7 Subsea Points - Removed (WGS84)\n",
      "8 Pipeline Points - Removed (WGS84)\n",
      "9 Pipeline Freespans - Removed (WGS84)\n",
      "10 Subsea Linear - Removed (WGS84)\n",
      "11 Pipelines Linear - Removed (WGS84)\n"
     ]
    }
   ],
   "source": [
    "# First get some layers that are not in the downloaded file\n",
    "\n",
    "get_arcgis_layers(ARCGIS_UK_INFRA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_remove = {1: 'uk_surface_points_removed',\n",
    "           2: 'uk_subsea_points_removed',\n",
    "           3: 'uk_pipeline_points_removed',\n",
    "           4: 'uk_pipeline_freespan_removed',\n",
    "           5: 'uk_subsea_linear_removed',\n",
    "           6: 'uk_pipelines_linear_removed'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dt/venv/gis/lib/python3.8/site-packages/geopandas/io/sql.py:400: UserWarning: Could not parse CRS from the GeoDataFrame. Inserting data without defined CRS.\n",
      "  srid = _get_srid_from_crs(gdf)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported uk_surface_points_removed to postgis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dt/venv/gis/lib/python3.8/site-packages/geopandas/io/sql.py:400: UserWarning: Could not parse CRS from the GeoDataFrame. Inserting data without defined CRS.\n",
      "  srid = _get_srid_from_crs(gdf)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported uk_subsea_points_removed to postgis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dt/venv/gis/lib/python3.8/site-packages/geopandas/io/sql.py:400: UserWarning: Could not parse CRS from the GeoDataFrame. Inserting data without defined CRS.\n",
      "  srid = _get_srid_from_crs(gdf)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported uk_pipeline_points_removed to postgis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dt/venv/gis/lib/python3.8/site-packages/geopandas/io/sql.py:400: UserWarning: Could not parse CRS from the GeoDataFrame. Inserting data without defined CRS.\n",
      "  srid = _get_srid_from_crs(gdf)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported uk_pipeline_freespan_removed to postgis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dt/venv/gis/lib/python3.8/site-packages/geopandas/io/sql.py:400: UserWarning: Could not parse CRS from the GeoDataFrame. Inserting data without defined CRS.\n",
      "  srid = _get_srid_from_crs(gdf)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported uk_subsea_linear_removed to postgis\n",
      "Exported uk_pipelines_linear_removed to postgis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dt/venv/gis/lib/python3.8/site-packages/geopandas/io/sql.py:400: UserWarning: Could not parse CRS from the GeoDataFrame. Inserting data without defined CRS.\n",
      "  srid = _get_srid_from_crs(gdf)\n"
     ]
    }
   ],
   "source": [
    "for key, name in to_remove.items():\n",
    "    df = arcgis2gdf(ARCGIS_UK_INFRA, key)\n",
    "    #df.crs('EPSG:4326')\n",
    "    #df = set_crs(df, UK_CRS, NEW_CRS)\n",
    "    df.columns = df.columns.str.lower()\n",
    "    df.to_postgis(name, POSTGIS_ENGINE, if_exists='replace')\n",
    "    print(f'Exported {name} to postgis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get lists of layers\n",
    "shps = []\n",
    "for file in glob.glob(f'{PATH_SPATIAL}UK/*.shp'):\n",
    "    shps.append(os.path.basename(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['UKCS_LicenceRelinquishments_ED50.shp',\n",
       " 'UKCS_Licences_ED50.shp',\n",
       " 'UKCS_Licensed_and_Unlicensed_Blocks_ED50.shp',\n",
       " 'UKCS_Licensed_Blocks_ED50.shp',\n",
       " 'UKCS_Licensed_Blocks_History_ED50.shp',\n",
       " 'UKCS_Offshore_FieldDets_ED50.shp',\n",
       " 'UKCS_Offshore_Fields_ED50.shp',\n",
       " 'UKCS_Pipelines_Linear_ED50.shp',\n",
       " 'UKCS_Pipeline_Freespans_ED50.shp',\n",
       " 'UKCS_Pipeline_Points_ED50.shp',\n",
       " 'UKCS_Quadrants_ED50.shp',\n",
       " 'UKCS_RestrictedBlocks_ED50.shp',\n",
       " 'UKCS_SubAreasByEqHold_ED50.shp',\n",
       " 'UKCS_SubAreas_ED50.shp',\n",
       " 'UKCS_Subsea_Linear_ED50.shp',\n",
       " 'UKCS_Subsea_Points_ED50.shp',\n",
       " 'UKCS_Surface_Points_ED50.shp',\n",
       " 'UKCS_Wells_ED50.shp',\n",
       " 'Well_Bottom_Holes_ED50.shp',\n",
       " 'Well_Paths_ED50.shp']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = []\n",
    "\n",
    "for name in shps:\n",
    "    name = name.lower().replace('ukcs_', 'uk_')[:-9]\n",
    "    names.append(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported uk_licencerelinquishments to postgis\n",
      "Exported uk_licences to postgis\n",
      "Exported uk_licensed_and_unlicensed_blocks to postgis\n",
      "Exported uk_licensed_blocks to postgis\n",
      "Exported uk_licensed_blocks_history to postgis\n",
      "Exported uk_offshore_fielddets to postgis\n",
      "Exported uk_offshore_fields to postgis\n",
      "Exported uk_pipelines_linear to postgis\n",
      "Exported uk_pipeline_freespans to postgis\n",
      "Exported uk_pipeline_points to postgis\n",
      "Exported uk_quadrants to postgis\n",
      "Exported uk_restrictedblocks to postgis\n",
      "Exported uk_subareasbyeqhold to postgis\n",
      "Exported uk_subareas to postgis\n",
      "Exported uk_subsea_linear to postgis\n",
      "Exported uk_subsea_points to postgis\n",
      "Exported uk_surface_points to postgis\n",
      "Exported uk_wells to postgis\n",
      "Exported well_bottom_holes to postgis\n",
      "Exported well_paths to postgis\n"
     ]
    }
   ],
   "source": [
    "for name, file in zip(names, shps):\n",
    "    \n",
    "    df = gpd.read_file(f'{PATH_SPATIAL}UK/{file}') \n",
    "    df = clean_uk(df)\n",
    "    df = df[df['geometry'] != None]\n",
    "    df.columns = df.columns.str.lower()\n",
    "    df.to_postgis(name, POSTGIS_ENGINE, if_exists='replace')\n",
    "    print(f'Exported {name} to postgis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "629"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import historical license data\n",
    "\n",
    "df = pd.read_excel('https://www.nstauthority.co.uk/media/7671/copy-of-2014-2020-field-equity-shares-june-2021.xlsx', skiprows=2)\n",
    "df.columns = df.columns.str.lower()\n",
    "df.to_sql('uk_historical_licences', POSTGIS_ENGINE, if_exists='replace')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import group names\n",
    "\n",
    "group_url = 'https://itportal.nstauthority.co.uk/eng/fox/oga-report/PED301X/companyLookup'\n",
    "df_groups = pd.read_html(group_url)[0]\n",
    "\n",
    "# Import companies with registration numbers\n",
    "\n",
    "company_url = 'https://itportal.nstauthority.co.uk/eng/fox/oga-report/PED301X/companyInfoDisplay'\n",
    "df_companies = pd.read_html(company_url)[0]\n",
    "\n",
    "# Merge groups and companies\n",
    "uk_companies = pd.merge(df_companies, df_groups, on='Name', how='left')\n",
    "#uk_companies.columns = uk_companies.columns.str.lower()\n",
    "# Write to postgis\n",
    "\n",
    "uk_companies.to_sql('uk_companies', POSTGIS_ENGINE, if_exists='append')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UK production data from January 1 2003\n",
    "\n",
    "'''For now create a manual download at:\n",
    "https://opendata-nstauthority.hub.arcgis.com/datasets/NSTAUTHORITY::-nsta-field-production-points-pprs-wgs84/explore?location=55.993346%2C-0.511550%2C6.89\n",
    "TODO: automate\n",
    "'''\n",
    "\n",
    "uk_production = gpd.read_file(f'{PATH_PRODUCTION}UK/uk_field_production/_NSTA_Field_Production%2C_PPRS_(WGS84).shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert datetime to proper date format\n",
    "\n",
    "uk_production['PERIODDATE'] = uk_production['PERIODDATE'].apply(pd.to_datetime, errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean UK production\n",
    "\n",
    "u = uk_production.copy()\n",
    "\n",
    "u.columns = u.columns.str.lower()\n",
    "\n",
    "u = u.melt(id_vars = ['fieldname', 'fieldarea', 'location', 'orggrpnm', \n",
    "            'unitname', 'unittypdes', 'perioddate', 'geometry'],\n",
    "           value_vars= u.columns[[14,22, 26]],\n",
    "           var_name = 'commodity',\n",
    "           value_name = 'sm3'\n",
    "          )\n",
    "\n",
    "sm3dict = {'dgas': 1000,\n",
    "           'gcond': 1000,}\n",
    "\n",
    "for k, v, in sm3dict.items():\n",
    "    u['sm3'] = np.where(u['commodity'].str.contains(k),\n",
    "                    u['sm3'] / v,\n",
    "                    u['sm3'])\n",
    "    \n",
    "u.commodity = u.commodity.str.replace('oilprodm3', 'Oil', regex=True)\n",
    "u.commodity = u.commodity.str.replace('dgasproksm', 'Gas', regex=True)\n",
    "u.commodity = u.commodity.str.replace('gcondvol', 'Condensate', regex=True)\n",
    "u.columns = u.columns.str.lower()\n",
    "\n",
    "u.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to postgis\n",
    "\n",
    "u.to_postgis('uk_production_monthly', POSTGIS_ENGINE, if_exists='append')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Netherlands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get pipelines\n",
    "\n",
    "get_wfs_layers(WFS_NL_INFRA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pipelines\n",
    "\n",
    "nl_pipelines = wfs2gdf(select_wfs_layer(WFS_NL_INFRA, 2), WFS_NL_INFRA, 'json')\n",
    "\n",
    "# And write to PostGIS\n",
    "\n",
    "#nl_pipelines.columns = nl_pipelines.columns.str.lower()\n",
    "\n",
    "nl_pipelines.to_postgis('nl_pipelines', POSTGIS_ENGINE, if_exists='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import licences\n",
    "\n",
    "nl_licences = wfs2gdf(select_wfs_layer(WFS_NL_LICENCES, 0), WFS_NL_LICENCES, 'application/gml+xml; version=3.2')\n",
    "\n",
    "# Write to PostGIS\n",
    "\n",
    "nl_licences.crs = 'EPSG:25831'\n",
    "nl_licences.columns = nl_licences.columns.str.lower()\n",
    "nl_licences.to_postgis('nl_licences_current', POSTGIS_ENGINE, if_exists='append')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nl_licences.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create table for licences per company\n",
    "\n",
    "nl_licences_company = nl_licences[['licence_nm', 'licensees']].copy()\n",
    "nl_licences_company.licensees = nl_licences_company.licensees.str.split(', ')\n",
    "nl_licences_company = nl_licences_company.explode('licensees')\n",
    "nl_licences_company.columns = nl_licences_company.columns.str.lower()\n",
    "\n",
    "nl_licences_company.to_sql('nl_licences_company_current', POSTGIS_ENGINE, if_exists='append')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nl_licence_hist = pd.read_excel(f'{PATH_LICENCES}NL/nl_production_licenses.xlsx', \n",
    "                                sheet_name='license_raw')\n",
    "\n",
    "# End date to datetime\n",
    "\n",
    "nl_licence_hist['end_date'] = nl_licence_hist['end_date'].apply(pd.to_datetime, errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to postgis\n",
    "\n",
    "nl_licence_hist.to_sql('nl_licence_hist', POSTGIS_ENGINE, if_exists='append')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Wellbores\n",
    "\n",
    "get_wfs_layers(WFS_NL_WELLBORES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Import\n",
    "\n",
    "wellbores_nl = wfs2gdf(select_wfs_layer(WFS_NL_WELLBORES, 0), WFS_NL_WELLBORES, 'application/gml+xml; version=3.2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform dates: GIVES AN ERROR WHEN WRITING TO POSTGIS SO SKIP FOR NOW\n",
    "\n",
    "date_cols = [col for col in wellbores_nl.columns if 'DATE' in col]\n",
    "\n",
    "wellbores_nl[date_cols] = wellbores_nl[date_cols].apply(pd.to_datetime, errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wellbores_nl.crs = 'EPSG:25831'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wellbores_nl.columns = wellbores_nl.columns.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to postgis\n",
    "\n",
    "wellbores_nl.to_postgis('nl_wellbores', POSTGIS_ENGINE, if_exists='append')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Facilities import and extract to folder\n",
    "\n",
    "nl_facility_url = 'https://www.nlog.nl/sites/default/files/2022-10/okt-2022-nlog-facility_utm.zip'\n",
    "\n",
    "r = requests.get(nl_facility_url)\n",
    "z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "z.extractall(f'{PATH_SPATIAL}NL/')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read file\n",
    "\n",
    "for file in glob.glob(f'{PATH_SPATIAL}NL/*NLOG-Facility_UTM.shp'):\n",
    "    nl_facility = gpd.read_file(file)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nl_facility.columns = nl_facility.columns.str.lower()\n",
    "\n",
    "# Write to PostGIS\n",
    "\n",
    "nl_facility.to_postgis('nl_facility', POSTGIS_ENGINE, if_exists='append')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Fields\n",
    "\n",
    "fields_nl = wfs2gdf(select_wfs_layer(WFS_NL_FIELD, 0), WFS_NL_FIELD, 'application/gml+xml; version=3.2')\n",
    "\n",
    "# Set CRS\n",
    "\n",
    "fields_nl.crs = 'EPSG:25831'\n",
    "\n",
    "# Transform dates\n",
    "\n",
    "fields_nl.DISCOVERY_DATE = fields_nl.DISCOVERY_DATE.apply(pd.to_datetime, errors='coerce')\n",
    "fields_nl.columns = fields_nl.columns.str.lower()\n",
    "\n",
    "# Write to PostGIS\n",
    "\n",
    "fields_nl.to_postgis('nl_fields', POSTGIS_ENGINE, if_exists='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get production data\n",
    "\n",
    "# Manual extract from https://www.nlog.nl/datacenter/prodfigures/fields)\n",
    "\n",
    "def parse_commodity(commodity):\n",
    "    dfs = []\n",
    "    for file in glob.glob(f'{PATH_PRODUCTION}NL/nl_production/per_field/field{commodity}Produced_*.xlsx'):\n",
    "        df = pd.read_excel(file, skiprows=1, skipfooter=1)\n",
    "        df = df.melt(['FIELD', 'OPERATOR', 'YEAR'], var_name='MONTH', value_name='sm3')\n",
    "        df['date'] = pd.to_datetime(df['YEAR'].astype(str) + '-' + df['MONTH'] + '-01')\n",
    "        if commodity == 'Oil':\n",
    "            df['sm3'] = df['sm3'] * 1000\n",
    "        df['COMMODITY'] = commodity\n",
    "        df.YEAR = df.YEAR.astype('int')\n",
    "        df = df.rename(columns={'LICENCE': 'license_name',\n",
    "                                'FIELD': 'license_name',\n",
    "                                'OPERATOR': 'operator_name',\n",
    "                                'YEAR': 'production_year',\n",
    "                                'MONTH': 'month',\n",
    "                                'COMMODITY': 'commodity'})\n",
    "\n",
    "        dfs.append(df)\n",
    "\n",
    "    df = pd.concat(dfs)\n",
    "    return df\n",
    "\n",
    "import warnings\n",
    "\n",
    "with warnings.catch_warnings(record=True):\n",
    "    warnings.simplefilter(\"always\")\n",
    "    df = pd.concat([parse_commodity('Gas'), parse_commodity('Oil'), parse_commodity('Condensate')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to PostGIS\n",
    "\n",
    "df.to_sql('nl_production_monthly', POSTGIS_ENGINE, if_exists='append')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EMODNET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get platforms\n",
    "\n",
    "emod = wfs2gdf(select_wfs_layer(EMODNET, 75), EMODNET, 'json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter North Sea countries\n",
    "\n",
    "emod = emod[emod.country.isin(['Norway', 'United Kingdom', 'Netherlands', 'Denmark', 'Germany'])].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to postgis\n",
    "\n",
    "emod.to_postgis('int_platforms', POSTGIS_ENGINE, if_exists='replace')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
